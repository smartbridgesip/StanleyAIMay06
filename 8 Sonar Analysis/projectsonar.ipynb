{"nbformat_minor": 2, "cells": [{"source": "\nimport types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share your notebook.\nclient_048c30274d2040b78af0dc08384d2c26 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='smcGpYE5_PHo2BOCe9Y55fRLz2aD6DamvSXhutPUPJ4c',\n    ibm_auth_endpoint=\"https://iam.bluemix.net/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.eu-geo.objectstorage.service.networklayer.com')\n\nbody = client_048c30274d2040b78af0dc08384d2c26.get_object(Bucket='gandhi-donotdelete-pr-aow6aqmfklaa1z',Key='Sonar edited.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndataset = pd.read_csv(body)\ndataset.head()\n\n", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "   20KHZ_200KHZ  200KHZ_2MHZ  2MHZ_20MHZ  20MHZ_200MHZ  200MHZ_2000MHZ  \\\n0       0.11506      0.24981     0.59690       0.54099         0.17553   \n1       0.18120      0.77810     0.36074       0.25250         0.07852   \n2       0.24274      0.69525     0.58549       0.48369         0.16200   \n3       0.05908      0.20185     0.38732       0.63428         0.30694   \n4       0.15241      0.46516     0.53996       0.33635         0.09049   \n\n   2000MHZ_2GHZ OBJECT  \n0       0.01108      R  \n1       0.00916      R  \n2       0.01603      R  \n3       0.00957      R  \n4       0.00792      R  ", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>20KHZ_200KHZ</th>\n      <th>200KHZ_2MHZ</th>\n      <th>2MHZ_20MHZ</th>\n      <th>20MHZ_200MHZ</th>\n      <th>200MHZ_2000MHZ</th>\n      <th>2000MHZ_2GHZ</th>\n      <th>OBJECT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.11506</td>\n      <td>0.24981</td>\n      <td>0.59690</td>\n      <td>0.54099</td>\n      <td>0.17553</td>\n      <td>0.01108</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.18120</td>\n      <td>0.77810</td>\n      <td>0.36074</td>\n      <td>0.25250</td>\n      <td>0.07852</td>\n      <td>0.00916</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.24274</td>\n      <td>0.69525</td>\n      <td>0.58549</td>\n      <td>0.48369</td>\n      <td>0.16200</td>\n      <td>0.01603</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.05908</td>\n      <td>0.20185</td>\n      <td>0.38732</td>\n      <td>0.63428</td>\n      <td>0.30694</td>\n      <td>0.00957</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.15241</td>\n      <td>0.46516</td>\n      <td>0.53996</td>\n      <td>0.33635</td>\n      <td>0.09049</td>\n      <td>0.00792</td>\n      <td>R</td>\n    </tr>\n  </tbody>\n</table>\n</div>"}, "execution_count": 1, "metadata": {}}], "execution_count": 1}, {"source": "import numpy as np\nimport matplotlib as plt\nprint(len(dataset.columns))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "7\n"}], "execution_count": 2}, {"source": "type(dataset)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "pandas.core.frame.DataFrame"}, "execution_count": 3, "metadata": {}}], "execution_count": 3}, {"source": "dataset", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "     20KHZ_200KHZ  200KHZ_2MHZ  2MHZ_20MHZ  20MHZ_200MHZ  200MHZ_2000MHZ  \\\n0         0.11506      0.24981     0.59690       0.54099         0.17553   \n1         0.18120      0.77810     0.36074       0.25250         0.07852   \n2         0.24274      0.69525     0.58549       0.48369         0.16200   \n3         0.05908      0.20185     0.38732       0.63428         0.30694   \n4         0.15241      0.46516     0.53996       0.33635         0.09049   \n5         0.10742      0.72656     0.40263       0.35433         0.11061   \n6         0.15114      0.33237     0.59969       0.47152         0.15838   \n7         0.10251      0.42242     0.64033       0.24333         0.13011   \n8         0.05817      0.34561     0.63844       0.28833         0.13613   \n9         0.04578      0.18975     0.80795       0.19636         0.07752   \n10        0.02627      0.10574     0.64931       0.54447         0.08005   \n11        0.04092      0.27744     0.69145       0.32461         0.07412   \n12        0.05095      0.39838     0.59291       0.20395         0.07745   \n13        0.08909      0.32255     0.59409       0.47686         0.15806   \n14        0.06608      0.31262     0.53542       0.39921         0.14018   \n15        0.12913      0.40270     0.54229       0.60019         0.13802   \n16        0.10652      0.26517     0.57489       0.58271         0.28012   \n17        0.07158      0.46206     0.45975       0.48300         0.21505   \n18        0.06432      0.31998     0.74057       0.56207         0.12470   \n19        0.25732      0.67712     0.55703       0.19045         0.09030   \n20        0.20865      0.38805     0.56794       0.67525         0.14982   \n21        0.09774      0.43081     0.36350       0.79929         0.26983   \n22        0.09613      0.20820     0.30924       0.74431         0.45254   \n23        0.04874      0.23414     0.61665       0.52236         0.15197   \n24        0.05696      0.14876     0.36934       0.65062         0.17424   \n25        0.03064      0.13600     0.75396       0.59843         0.20975   \n26        0.09909      0.47847     0.62573       0.17467         0.12054   \n27        0.04873      0.43374     0.72837       0.24313         0.11566   \n28        0.07082      0.47290     0.56024       0.34349         0.11991   \n29        0.09387      0.37372     0.58909       0.15119         0.09446   \n..            ...          ...         ...           ...             ...   \n178       0.06308      0.25819     0.73458       0.35789         0.09518   \n179       0.09758      0.31731     0.77146       0.38765         0.15614   \n180       0.13990      0.41413     0.78911       0.33615         0.21311   \n181       0.11046      0.35166     0.73963       0.41534         0.39845   \n182       0.11752      0.42436     0.74824       0.42688         0.28494   \n183       0.12282      0.42851     0.81525       0.42225         0.28917   \n184       0.19577      0.45557     0.78812       0.31479         0.23098   \n185       0.12972      0.45195     0.82363       0.37695         0.24957   \n186       0.13211      0.32343     0.62144       0.39482         0.43466   \n187       0.09905      0.27023     0.63169       0.29965         0.22844   \n188       0.08317      0.25533     0.65294       0.29286         0.18982   \n189       0.09756      0.28500     0.69206       0.27582         0.14391   \n190       0.07235      0.25724     0.69551       0.30166         0.14937   \n191       0.08179      0.24431     0.70737       0.26832         0.14526   \n192       0.08421      0.24814     0.69560       0.25114         0.12751   \n193       0.06999      0.21620     0.69992       0.30362         0.14761   \n194       0.09244      0.26033     0.82021       0.24616         0.11037   \n195       0.08300      0.32391     0.85677       0.18710         0.16158   \n196       0.08933      0.35091     0.89118       0.16095         0.13809   \n197       0.07604      0.30736     0.86056       0.19538         0.13693   \n198       0.08597      0.29051     0.84941       0.17826         0.13029   \n199       0.08231      0.34193     0.86991       0.20376         0.09609   \n200       0.12318      0.36669     0.87394       0.18391         0.13956   \n201       0.11099      0.35770     0.84410       0.13878         0.13301   \n202       0.15012      0.24757     0.74671       0.28210         0.08227   \n203       0.11635      0.22679     0.80091       0.23933         0.11572   \n204       0.08196      0.22516     0.82320       0.28966         0.09134   \n205       0.09175      0.17556     0.79874       0.29309         0.08165   \n206       0.10162      0.21514     0.79234       0.22140         0.10205   \n207       0.07835      0.23791     0.81887       0.25058         0.11501   \n\n     2000MHZ_2GHZ OBJECT  \n0         0.01108      R  \n1         0.00916      R  \n2         0.01603      R  \n3         0.00957      R  \n4         0.00792      R  \n5         0.00500      R  \n6         0.01357      R  \n7         0.00749      R  \n8         0.00825      R  \n9         0.00925      R  \n10        0.00579      R  \n11        0.01173      R  \n12        0.00890      R  \n13        0.01269      R  \n14        0.01153      R  \n15        0.01171      R  \n16        0.01491      R  \n17        0.01343      R  \n18        0.00674      R  \n19        0.00959      R  \n20        0.00764      R  \n21        0.00945      R  \n22        0.01603      R  \n23        0.00565      R  \n24        0.00864      R  \n25        0.00486      R  \n26        0.00543      R  \n27        0.00842      R  \n28        0.00767      R  \n29        0.01015      R  \n..            ...    ...  \n178       0.00681      M  \n179       0.00821      M  \n180       0.00873      M  \n181       0.01004      M  \n182       0.00968      M  \n183       0.01157      M  \n184       0.01165      M  \n185       0.00896      M  \n186       0.00768      M  \n187       0.00704      M  \n188       0.00728      M  \n189       0.00739      M  \n190       0.00794      M  \n191       0.00576      M  \n192       0.00640      M  \n193       0.00664      M  \n194       0.00550      M  \n195       0.00663      M  \n196       0.00733      M  \n197       0.00518      M  \n198       0.00637      M  \n199       0.00779      M  \n200       0.00652      M  \n201       0.00553      M  \n202       0.00739      M  \n203       0.01280      M  \n204       0.00661      M  \n205       0.00932      M  \n206       0.00568      M  \n207       0.00855      M  \n\n[208 rows x 7 columns]", "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>20KHZ_200KHZ</th>\n      <th>200KHZ_2MHZ</th>\n      <th>2MHZ_20MHZ</th>\n      <th>20MHZ_200MHZ</th>\n      <th>200MHZ_2000MHZ</th>\n      <th>2000MHZ_2GHZ</th>\n      <th>OBJECT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.11506</td>\n      <td>0.24981</td>\n      <td>0.59690</td>\n      <td>0.54099</td>\n      <td>0.17553</td>\n      <td>0.01108</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.18120</td>\n      <td>0.77810</td>\n      <td>0.36074</td>\n      <td>0.25250</td>\n      <td>0.07852</td>\n      <td>0.00916</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.24274</td>\n      <td>0.69525</td>\n      <td>0.58549</td>\n      <td>0.48369</td>\n      <td>0.16200</td>\n      <td>0.01603</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.05908</td>\n      <td>0.20185</td>\n      <td>0.38732</td>\n      <td>0.63428</td>\n      <td>0.30694</td>\n      <td>0.00957</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.15241</td>\n      <td>0.46516</td>\n      <td>0.53996</td>\n      <td>0.33635</td>\n      <td>0.09049</td>\n      <td>0.00792</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.10742</td>\n      <td>0.72656</td>\n      <td>0.40263</td>\n      <td>0.35433</td>\n      <td>0.11061</td>\n      <td>0.00500</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.15114</td>\n      <td>0.33237</td>\n      <td>0.59969</td>\n      <td>0.47152</td>\n      <td>0.15838</td>\n      <td>0.01357</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.10251</td>\n      <td>0.42242</td>\n      <td>0.64033</td>\n      <td>0.24333</td>\n      <td>0.13011</td>\n      <td>0.00749</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.05817</td>\n      <td>0.34561</td>\n      <td>0.63844</td>\n      <td>0.28833</td>\n      <td>0.13613</td>\n      <td>0.00825</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.04578</td>\n      <td>0.18975</td>\n      <td>0.80795</td>\n      <td>0.19636</td>\n      <td>0.07752</td>\n      <td>0.00925</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.02627</td>\n      <td>0.10574</td>\n      <td>0.64931</td>\n      <td>0.54447</td>\n      <td>0.08005</td>\n      <td>0.00579</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.04092</td>\n      <td>0.27744</td>\n      <td>0.69145</td>\n      <td>0.32461</td>\n      <td>0.07412</td>\n      <td>0.01173</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.05095</td>\n      <td>0.39838</td>\n      <td>0.59291</td>\n      <td>0.20395</td>\n      <td>0.07745</td>\n      <td>0.00890</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.08909</td>\n      <td>0.32255</td>\n      <td>0.59409</td>\n      <td>0.47686</td>\n      <td>0.15806</td>\n      <td>0.01269</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.06608</td>\n      <td>0.31262</td>\n      <td>0.53542</td>\n      <td>0.39921</td>\n      <td>0.14018</td>\n      <td>0.01153</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.12913</td>\n      <td>0.40270</td>\n      <td>0.54229</td>\n      <td>0.60019</td>\n      <td>0.13802</td>\n      <td>0.01171</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.10652</td>\n      <td>0.26517</td>\n      <td>0.57489</td>\n      <td>0.58271</td>\n      <td>0.28012</td>\n      <td>0.01491</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.07158</td>\n      <td>0.46206</td>\n      <td>0.45975</td>\n      <td>0.48300</td>\n      <td>0.21505</td>\n      <td>0.01343</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.06432</td>\n      <td>0.31998</td>\n      <td>0.74057</td>\n      <td>0.56207</td>\n      <td>0.12470</td>\n      <td>0.00674</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.25732</td>\n      <td>0.67712</td>\n      <td>0.55703</td>\n      <td>0.19045</td>\n      <td>0.09030</td>\n      <td>0.00959</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.20865</td>\n      <td>0.38805</td>\n      <td>0.56794</td>\n      <td>0.67525</td>\n      <td>0.14982</td>\n      <td>0.00764</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.09774</td>\n      <td>0.43081</td>\n      <td>0.36350</td>\n      <td>0.79929</td>\n      <td>0.26983</td>\n      <td>0.00945</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.09613</td>\n      <td>0.20820</td>\n      <td>0.30924</td>\n      <td>0.74431</td>\n      <td>0.45254</td>\n      <td>0.01603</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.04874</td>\n      <td>0.23414</td>\n      <td>0.61665</td>\n      <td>0.52236</td>\n      <td>0.15197</td>\n      <td>0.00565</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.05696</td>\n      <td>0.14876</td>\n      <td>0.36934</td>\n      <td>0.65062</td>\n      <td>0.17424</td>\n      <td>0.00864</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.03064</td>\n      <td>0.13600</td>\n      <td>0.75396</td>\n      <td>0.59843</td>\n      <td>0.20975</td>\n      <td>0.00486</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.09909</td>\n      <td>0.47847</td>\n      <td>0.62573</td>\n      <td>0.17467</td>\n      <td>0.12054</td>\n      <td>0.00543</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.04873</td>\n      <td>0.43374</td>\n      <td>0.72837</td>\n      <td>0.24313</td>\n      <td>0.11566</td>\n      <td>0.00842</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.07082</td>\n      <td>0.47290</td>\n      <td>0.56024</td>\n      <td>0.34349</td>\n      <td>0.11991</td>\n      <td>0.00767</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.09387</td>\n      <td>0.37372</td>\n      <td>0.58909</td>\n      <td>0.15119</td>\n      <td>0.09446</td>\n      <td>0.01015</td>\n      <td>R</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>178</th>\n      <td>0.06308</td>\n      <td>0.25819</td>\n      <td>0.73458</td>\n      <td>0.35789</td>\n      <td>0.09518</td>\n      <td>0.00681</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>179</th>\n      <td>0.09758</td>\n      <td>0.31731</td>\n      <td>0.77146</td>\n      <td>0.38765</td>\n      <td>0.15614</td>\n      <td>0.00821</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>180</th>\n      <td>0.13990</td>\n      <td>0.41413</td>\n      <td>0.78911</td>\n      <td>0.33615</td>\n      <td>0.21311</td>\n      <td>0.00873</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>181</th>\n      <td>0.11046</td>\n      <td>0.35166</td>\n      <td>0.73963</td>\n      <td>0.41534</td>\n      <td>0.39845</td>\n      <td>0.01004</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>182</th>\n      <td>0.11752</td>\n      <td>0.42436</td>\n      <td>0.74824</td>\n      <td>0.42688</td>\n      <td>0.28494</td>\n      <td>0.00968</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.12282</td>\n      <td>0.42851</td>\n      <td>0.81525</td>\n      <td>0.42225</td>\n      <td>0.28917</td>\n      <td>0.01157</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.19577</td>\n      <td>0.45557</td>\n      <td>0.78812</td>\n      <td>0.31479</td>\n      <td>0.23098</td>\n      <td>0.01165</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>185</th>\n      <td>0.12972</td>\n      <td>0.45195</td>\n      <td>0.82363</td>\n      <td>0.37695</td>\n      <td>0.24957</td>\n      <td>0.00896</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>186</th>\n      <td>0.13211</td>\n      <td>0.32343</td>\n      <td>0.62144</td>\n      <td>0.39482</td>\n      <td>0.43466</td>\n      <td>0.00768</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>187</th>\n      <td>0.09905</td>\n      <td>0.27023</td>\n      <td>0.63169</td>\n      <td>0.29965</td>\n      <td>0.22844</td>\n      <td>0.00704</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>0.08317</td>\n      <td>0.25533</td>\n      <td>0.65294</td>\n      <td>0.29286</td>\n      <td>0.18982</td>\n      <td>0.00728</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>0.09756</td>\n      <td>0.28500</td>\n      <td>0.69206</td>\n      <td>0.27582</td>\n      <td>0.14391</td>\n      <td>0.00739</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>0.07235</td>\n      <td>0.25724</td>\n      <td>0.69551</td>\n      <td>0.30166</td>\n      <td>0.14937</td>\n      <td>0.00794</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>0.08179</td>\n      <td>0.24431</td>\n      <td>0.70737</td>\n      <td>0.26832</td>\n      <td>0.14526</td>\n      <td>0.00576</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>0.08421</td>\n      <td>0.24814</td>\n      <td>0.69560</td>\n      <td>0.25114</td>\n      <td>0.12751</td>\n      <td>0.00640</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>0.06999</td>\n      <td>0.21620</td>\n      <td>0.69992</td>\n      <td>0.30362</td>\n      <td>0.14761</td>\n      <td>0.00664</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>0.09244</td>\n      <td>0.26033</td>\n      <td>0.82021</td>\n      <td>0.24616</td>\n      <td>0.11037</td>\n      <td>0.00550</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>0.08300</td>\n      <td>0.32391</td>\n      <td>0.85677</td>\n      <td>0.18710</td>\n      <td>0.16158</td>\n      <td>0.00663</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>0.08933</td>\n      <td>0.35091</td>\n      <td>0.89118</td>\n      <td>0.16095</td>\n      <td>0.13809</td>\n      <td>0.00733</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>0.07604</td>\n      <td>0.30736</td>\n      <td>0.86056</td>\n      <td>0.19538</td>\n      <td>0.13693</td>\n      <td>0.00518</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>0.08597</td>\n      <td>0.29051</td>\n      <td>0.84941</td>\n      <td>0.17826</td>\n      <td>0.13029</td>\n      <td>0.00637</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>0.08231</td>\n      <td>0.34193</td>\n      <td>0.86991</td>\n      <td>0.20376</td>\n      <td>0.09609</td>\n      <td>0.00779</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>200</th>\n      <td>0.12318</td>\n      <td>0.36669</td>\n      <td>0.87394</td>\n      <td>0.18391</td>\n      <td>0.13956</td>\n      <td>0.00652</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>201</th>\n      <td>0.11099</td>\n      <td>0.35770</td>\n      <td>0.84410</td>\n      <td>0.13878</td>\n      <td>0.13301</td>\n      <td>0.00553</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>202</th>\n      <td>0.15012</td>\n      <td>0.24757</td>\n      <td>0.74671</td>\n      <td>0.28210</td>\n      <td>0.08227</td>\n      <td>0.00739</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>203</th>\n      <td>0.11635</td>\n      <td>0.22679</td>\n      <td>0.80091</td>\n      <td>0.23933</td>\n      <td>0.11572</td>\n      <td>0.01280</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>204</th>\n      <td>0.08196</td>\n      <td>0.22516</td>\n      <td>0.82320</td>\n      <td>0.28966</td>\n      <td>0.09134</td>\n      <td>0.00661</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>205</th>\n      <td>0.09175</td>\n      <td>0.17556</td>\n      <td>0.79874</td>\n      <td>0.29309</td>\n      <td>0.08165</td>\n      <td>0.00932</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>206</th>\n      <td>0.10162</td>\n      <td>0.21514</td>\n      <td>0.79234</td>\n      <td>0.22140</td>\n      <td>0.10205</td>\n      <td>0.00568</td>\n      <td>M</td>\n    </tr>\n    <tr>\n      <th>207</th>\n      <td>0.07835</td>\n      <td>0.23791</td>\n      <td>0.81887</td>\n      <td>0.25058</td>\n      <td>0.11501</td>\n      <td>0.00855</td>\n      <td>M</td>\n    </tr>\n  </tbody>\n</table>\n<p>208 rows \u00d7 7 columns</p>\n</div>"}, "execution_count": 4, "metadata": {}}], "execution_count": 4}, {"source": "X=dataset.iloc[:,0:6].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 5}, {"source": "X", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 0.11506,  0.24981,  0.5969 ,  0.54099,  0.17553,  0.01108],\n       [ 0.1812 ,  0.7781 ,  0.36074,  0.2525 ,  0.07852,  0.00916],\n       [ 0.24274,  0.69525,  0.58549,  0.48369,  0.162  ,  0.01603],\n       ..., \n       [ 0.09175,  0.17556,  0.79874,  0.29309,  0.08165,  0.00932],\n       [ 0.10162,  0.21514,  0.79234,  0.2214 ,  0.10205,  0.00568],\n       [ 0.07835,  0.23791,  0.81887,  0.25058,  0.11501,  0.00855]])"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "y = dataset.iloc[:,-1].values", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array(['R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R', 'R',\n       'R', 'R', 'R', 'R', 'R', 'R', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M',\n       'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M'], dtype=object)"}, "execution_count": 8, "metadata": {}}], "execution_count": 8}, {"source": "#encode the dependent variable containing categorical values\nfrom sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 9}, {"source": "encoder.fit(y)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "LabelEncoder()"}, "execution_count": 10, "metadata": {}}], "execution_count": 10}, {"source": "y = encoder.transform(y)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 11}, {"source": "y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0])"}, "execution_count": 12, "metadata": {}}], "execution_count": 12}, {"source": "#transform the data in training and testing\nfrom sklearn.utils import shuffle\nX,y = shuffle(X,y,random_state=1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 13}, {"source": "from sklearn.model_selection import train_test_split\ntrain_x,test_x,train_y,test_y = train_test_split(X,y,test_size=0.20, random_state=42)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 14}, {"source": "from sklearn.preprocessing import StandardScaler\nsc= StandardScaler()\ntrain_x =sc.fit_transform(train_x)\ntest_x = sc.transform(test_x)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 15}, {"source": "import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "Using TensorFlow backend.\n"}], "execution_count": 16}, {"source": "keras.__version__", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'2.1.4'"}, "execution_count": 17, "metadata": {}}], "execution_count": 17}, {"source": "model=Sequential()", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 18}, {"source": "train_x.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(166, 6)"}, "execution_count": 19, "metadata": {}}], "execution_count": 19}, {"source": "train_y.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(166,)"}, "execution_count": 20, "metadata": {}}], "execution_count": 20}, {"source": "model.add(Dense(input_dim=6,init=\"random_uniform\",activation='relu',output_dim=15))#input layer", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=15, kernel_initializer=\"random_uniform\", activation=\"relu\", input_dim=6)`\n  if __name__ == '__main__':\n"}], "execution_count": 21}, {"source": "model.add(Dense(output_dim=8,init=\"random_uniform\",activation='relu'))#hidden layer", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"random_uniform\", units=8, activation=\"relu\")`\n  if __name__ == '__main__':\n"}], "execution_count": 22}, {"source": "model.add(Dense(output_dim=1,init='random_uniform',activation='sigmoid'))#output layer", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/ipykernel/__main__.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(kernel_initializer=\"random_uniform\", units=1, activation=\"sigmoid\")`\n  if __name__ == '__main__':\n"}], "execution_count": 23}, {"source": "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])#adam=batch gradent descent", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 24}, {"source": "#train_y.reshape(1,-1)\ntrain_y.shape", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(166,)"}, "execution_count": 25, "metadata": {}}], "execution_count": 25}, {"source": "model.fit(train_x,train_y,epochs=108, batch_size=83)#epochs no of iteration ", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Epoch 1/108\n166/166 [==============================] - 0s 664us/step - loss: 0.6931 - acc: 0.4578\nEpoch 2/108\n166/166 [==============================] - 0s 30us/step - loss: 0.6930 - acc: 0.5361\nEpoch 3/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6930 - acc: 0.5361\nEpoch 4/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6929 - acc: 0.5361\nEpoch 5/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6928 - acc: 0.5361\nEpoch 6/108\n166/166 [==============================] - 0s 20us/step - loss: 0.6927 - acc: 0.5361\nEpoch 7/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6927 - acc: 0.5361\nEpoch 8/108\n166/166 [==============================] - 0s 374us/step - loss: 0.6926 - acc: 0.5361\nEpoch 9/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6925 - acc: 0.5361\nEpoch 10/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6924 - acc: 0.5361\nEpoch 11/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6923 - acc: 0.5361\nEpoch 12/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6921 - acc: 0.5361\nEpoch 13/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6920 - acc: 0.5361\nEpoch 14/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6918 - acc: 0.5361\nEpoch 15/108\n166/166 [==============================] - 0s 27us/step - loss: 0.6917 - acc: 0.5361\nEpoch 16/108\n166/166 [==============================] - 0s 414us/step - loss: 0.6915 - acc: 0.5361\nEpoch 17/108\n166/166 [==============================] - 0s 32us/step - loss: 0.6913 - acc: 0.5361\nEpoch 18/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6910 - acc: 0.5361\nEpoch 19/108\n166/166 [==============================] - 0s 27us/step - loss: 0.6908 - acc: 0.5361\nEpoch 20/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6905 - acc: 0.5422\nEpoch 21/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6901 - acc: 0.5422\nEpoch 22/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6897 - acc: 0.5663\nEpoch 23/108\n166/166 [==============================] - ETA: 0s - loss: 0.6887 - acc: 0.614 - 0s 23us/step - loss: 0.6893 - acc: 0.5904\nEpoch 24/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6888 - acc: 0.6205\nEpoch 25/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6883 - acc: 0.6386\nEpoch 26/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6878 - acc: 0.6747\nEpoch 27/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6871 - acc: 0.6928\nEpoch 28/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6864 - acc: 0.7048\nEpoch 29/108\n166/166 [==============================] - 0s 28us/step - loss: 0.6857 - acc: 0.7289\nEpoch 30/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6849 - acc: 0.7349\nEpoch 31/108\n166/166 [==============================] - 0s 29us/step - loss: 0.6839 - acc: 0.7530\nEpoch 32/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6829 - acc: 0.7349\nEpoch 33/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6818 - acc: 0.7349\nEpoch 34/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6806 - acc: 0.7229\nEpoch 35/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6793 - acc: 0.7169\nEpoch 36/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6779 - acc: 0.7169\nEpoch 37/108\n166/166 [==============================] - ETA: 0s - loss: 0.6764 - acc: 0.722 - 0s 423us/step - loss: 0.6766 - acc: 0.7169\nEpoch 38/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6750 - acc: 0.7229\nEpoch 39/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6732 - acc: 0.7169\nEpoch 40/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6715 - acc: 0.7108\nEpoch 41/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6696 - acc: 0.7229\nEpoch 42/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6677 - acc: 0.7289\nEpoch 43/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6656 - acc: 0.7410\nEpoch 44/108\n166/166 [==============================] - 0s 449us/step - loss: 0.6633 - acc: 0.7410\nEpoch 45/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6612 - acc: 0.7410\nEpoch 46/108\n166/166 [==============================] - 0s 21us/step - loss: 0.6587 - acc: 0.7470\nEpoch 47/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6562 - acc: 0.7530\nEpoch 48/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6537 - acc: 0.7590\nEpoch 49/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6509 - acc: 0.7590\nEpoch 50/108\n166/166 [==============================] - 0s 444us/step - loss: 0.6479 - acc: 0.7590\nEpoch 51/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6453 - acc: 0.7530\nEpoch 52/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6424 - acc: 0.7470\nEpoch 53/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6394 - acc: 0.7530\nEpoch 54/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6362 - acc: 0.7470\nEpoch 55/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6328 - acc: 0.7410\nEpoch 56/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6300 - acc: 0.7470\nEpoch 57/108\n166/166 [==============================] - 0s 431us/step - loss: 0.6266 - acc: 0.7470\nEpoch 58/108\n166/166 [==============================] - 0s 24us/step - loss: 0.6235 - acc: 0.7530\nEpoch 59/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6200 - acc: 0.7530\nEpoch 60/108\n166/166 [==============================] - 0s 26us/step - loss: 0.6168 - acc: 0.7651\nEpoch 61/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6136 - acc: 0.7651\nEpoch 62/108\n166/166 [==============================] - 0s 23us/step - loss: 0.6104 - acc: 0.7711\nEpoch 63/108\n166/166 [==============================] - 0s 22us/step - loss: 0.6073 - acc: 0.7711\nEpoch 64/108\n166/166 [==============================] - 0s 432us/step - loss: 0.6040 - acc: 0.7711\nEpoch 65/108\n166/166 [==============================] - 0s 25us/step - loss: 0.6006 - acc: 0.7711\nEpoch 66/108\n166/166 [==============================] - 0s 21us/step - loss: 0.5976 - acc: 0.7711\nEpoch 67/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5946 - acc: 0.7711\nEpoch 68/108\n166/166 [==============================] - 0s 25us/step - loss: 0.5914 - acc: 0.7711\nEpoch 69/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5883 - acc: 0.7711\nEpoch 70/108\n166/166 [==============================] - 0s 25us/step - loss: 0.5853 - acc: 0.7711\nEpoch 71/108\n166/166 [==============================] - 0s 439us/step - loss: 0.5823 - acc: 0.7711\nEpoch 72/108\n166/166 [==============================] - 0s 24us/step - loss: 0.5793 - acc: 0.7590\nEpoch 73/108\n166/166 [==============================] - 0s 24us/step - loss: 0.5761 - acc: 0.7590\nEpoch 74/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5732 - acc: 0.7590\nEpoch 75/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5705 - acc: 0.7590\nEpoch 76/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5673 - acc: 0.7590\nEpoch 77/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5644 - acc: 0.7590\nEpoch 78/108\n166/166 [==============================] - 0s 418us/step - loss: 0.5613 - acc: 0.7590\nEpoch 79/108\n166/166 [==============================] - 0s 24us/step - loss: 0.5588 - acc: 0.7590\nEpoch 80/108\n166/166 [==============================] - 0s 25us/step - loss: 0.5559 - acc: 0.7651\nEpoch 81/108\n166/166 [==============================] - 0s 27us/step - loss: 0.5532 - acc: 0.7711\nEpoch 82/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5505 - acc: 0.7711\nEpoch 83/108\n"}, {"output_type": "stream", "name": "stdout", "text": "166/166 [==============================] - 0s 26us/step - loss: 0.5479 - acc: 0.7711\nEpoch 84/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5456 - acc: 0.7711\nEpoch 85/108\n166/166 [==============================] - 0s 424us/step - loss: 0.5430 - acc: 0.7711\nEpoch 86/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5403 - acc: 0.7711\nEpoch 87/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5382 - acc: 0.7711\nEpoch 88/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5354 - acc: 0.7711\nEpoch 89/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5331 - acc: 0.7711\nEpoch 90/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5307 - acc: 0.7711\nEpoch 91/108\n166/166 [==============================] - 0s 20us/step - loss: 0.5284 - acc: 0.7711\nEpoch 92/108\n166/166 [==============================] - 0s 21us/step - loss: 0.5261 - acc: 0.7711\nEpoch 93/108\n166/166 [==============================] - 0s 428us/step - loss: 0.5238 - acc: 0.7771\nEpoch 94/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5215 - acc: 0.7771\nEpoch 95/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5192 - acc: 0.7771\nEpoch 96/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5168 - acc: 0.7771\nEpoch 97/108\n166/166 [==============================] - 0s 24us/step - loss: 0.5146 - acc: 0.7831\nEpoch 98/108\n166/166 [==============================] - 0s 24us/step - loss: 0.5123 - acc: 0.7892\nEpoch 99/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5101 - acc: 0.7892\nEpoch 100/108\n166/166 [==============================] - 0s 423us/step - loss: 0.5080 - acc: 0.7892\nEpoch 101/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5058 - acc: 0.7831\nEpoch 102/108\n166/166 [==============================] - 0s 23us/step - loss: 0.5037 - acc: 0.7952\nEpoch 103/108\n166/166 [==============================] - 0s 22us/step - loss: 0.5016 - acc: 0.7952\nEpoch 104/108\n166/166 [==============================] - 0s 21us/step - loss: 0.4993 - acc: 0.7952\nEpoch 105/108\n166/166 [==============================] - 0s 21us/step - loss: 0.4975 - acc: 0.7952\nEpoch 106/108\n166/166 [==============================] - 0s 21us/step - loss: 0.4953 - acc: 0.7952\nEpoch 107/108\n166/166 [==============================] - 0s 430us/step - loss: 0.4933 - acc: 0.7952\nEpoch 108/108\n166/166 [==============================] - 0s 22us/step - loss: 0.4914 - acc: 0.8072\n"}, {"output_type": "execute_result", "data": {"text/plain": "<keras.callbacks.History at 0x7faa9343ad30>"}, "execution_count": 26, "metadata": {}}], "execution_count": 26}, {"source": "y_pred = model.predict(test_x)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 27}, {"source": "y_pred=(y_pred>0.5)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 28}, {"source": "y_pred", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ True],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [ True],\n       [ True],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [False],\n       [ True],\n       [False],\n       [False],\n       [ True],\n       [ True],\n       [ True]], dtype=bool)"}, "execution_count": 29, "metadata": {}}], "execution_count": 29}, {"source": "test_y", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n       1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1])"}, "execution_count": 30, "metadata": {}}], "execution_count": 30}, {"source": "y_p=model.predict(sc.transform(np.array([[0.04708,0.23187,0.56334,0.4592,0.11611,0.00494]])))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 48}, {"source": "y_p", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ 0.9324683]], dtype=float32)"}, "execution_count": 49, "metadata": {}}], "execution_count": 49}, {"source": "y_p=(y_p>0.4999607)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 33}, {"source": "y_p", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[ True]], dtype=bool)"}, "execution_count": 34, "metadata": {}}], "execution_count": 34}, {"source": "from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(test_y,y_pred)\nprint(cm)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "[[20  2]\n [ 4 16]]\n"}], "execution_count": 35}, {"source": "\nfrom sklearn.metrics import accuracy_score\naccuracy_score(test_y, y_pred)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.8571428571428571"}, "execution_count": 36, "metadata": {}}], "execution_count": 36}, {"source": "model.save(\"projects.h5\")", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 37}, {"source": "import os\ncwd = os.getcwd()\ncwd", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'/home/dsxuser/work'"}, "execution_count": 38, "metadata": {}}], "execution_count": 38}, {"source": "!tar -zcvf projects.tgz projects.h5", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "projects.h5\r\n"}], "execution_count": 39}, {"source": "from watson_machine_learning_client import WatsonMachineLearningAPIClient", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "/opt/conda/envs/DSX-Python35/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n"}], "execution_count": 40}, {"source": "wml_credentials={\n    \"url\": \"https://eu-gb.ml.cloud.ibm.com\",\n    \"instance_id\": \"e1f71393-defe-402f-948e-a1a2b9914e02\",\n    \"username\": \"b9c424e1-f4b7-4798-9fae-68d2ce88ce37\",\n    \"password\": \"bc54ef99-dee5-41ed-9958-625494961f35\",\n     \"access_key\": \"_SdcBeC8ZGvinPVi3hduq7cNfmZJMuU50hp8igQ-u-FV\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 41}, {"source": "Client = WatsonMachineLearningAPIClient(wml_credentials)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 42}, {"source": "metadata={\nClient.repository.ModelMetaNames.NAME: \"keras\",\nClient.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{'name':'keras','version': '2.1.3'}],\nClient.repository.ModelMetaNames.FRAMEWORK_VERSION: '1.5',\nClient.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\"\n}", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 43}, {"source": "model_details=Client.repository.store_model(model=\"projects.tgz\",meta_props=metadata)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 44}, {"source": "model_id=model_details[\"metadata\"][\"guid\"]\nmodel_deployment_details=Client.deployments.create(artifact_uid=model_id,name='deployment')", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "\n\n#######################################################################################\n\nSynchronous deployment creation for uid: 'e21d4e03-21e6-4556-946a-2f333a3ad901' started\n\n#######################################################################################\n\n\nINITIALIZING\nDEPLOY_SUCCESS\n\n\n------------------------------------------------------------------------------------------------\nSuccessfully finished deployment creation, deployment_uid='73f12bdd-68aa-4b15-a3d1-a8f65fffb5d5'\n------------------------------------------------------------------------------------------------\n\n\n"}], "execution_count": 45}, {"source": "scoring_endpoint=Client.deployments.get_scoring_url(model_deployment_details)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 46}, {"source": "scoring_endpoint", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'https://eu-gb.ml.cloud.ibm.com/v3/wml_instances/e1f71393-defe-402f-948e-a1a2b9914e02/deployments/73f12bdd-68aa-4b15-a3d1-a8f65fffb5d5/online'"}, "execution_count": 47, "metadata": {}}], "execution_count": 47}, {"source": "", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": null}], "metadata": {"kernelspec": {"display_name": "Python 3.5", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.5", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}